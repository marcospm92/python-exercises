{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 12\n",
    "\n",
    "## Enunciado\n",
    "Crea un programa que:\n",
    "\n",
    "1. Acceda al [siguiente enlace](https://www.elmundo.es/ciencia-y-salud/salud/2020/03/30/5e817ba4fdddff906a8b4575.html) y obtenga los datos referentes al coronavirus en España.\n",
    "2. Imprima la información por pantalla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué cosas nuevas necesitamos saber?\n",
    "- Instalar librerías con pip.\n",
    "- Uso de las librerias **requests** y **bs4** para scraping de páginas web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar librerías con pip.\n",
    "\n",
    "Lo primero de todo, igual que hicimos con jupyterlab, necesitamos instalar un par de librerías adicionales con pip.\n",
    "\n",
    "**Windows**:\n",
    "Ejecutamos PowerShell como Administrados y ejecutamos la siguiente sentencia:\n",
    "```consoles\n",
    "pip install -U requests bs4\n",
    "```\n",
    "\n",
    "**Linux o MacOS**:\n",
    "Abrimos una Terminal y ejecutamos la siguiente sentencia:\n",
    "```console\n",
    "sudo pip install -U requests bs4\n",
    "```\n",
    "\n",
    "Una vez hecho esto, estaremos en disposición de ejecutar las siguientes celdas de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de las librerias **requests** y **bs4** para scraping de páginas web.\n",
    "\n",
    "Una vez realizado los imports, veamos como pedir el contenido de una página web.\n",
    "\n",
    "Utilizaremos [Wikipedia - Don Quijote De La Mancha](https://es.wikipedia.org/wiki/Don_Quijote_de_la_Mancha) como referencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://es.wikipedia.org/wiki/Don_Quijote_de_la_Mancha\" # guardamos en una variable el enlace web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(url) # con el método get, solicitamos la web como cuando hacemos clic en el enlace (más o menos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = req.text # mediante el atributo text, obtenemos el código fuente de la página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veamos que pinta tiene\n",
    "# como el código es muy grande, veremos solo un fragmento...\n",
    "print(source[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código es el mismo que se obtiene pulsando botón derecho en cualquier parte de la página web y haciendo clic sobre **Ver código fuente de la página\" o **CTRL + U**.\n",
    "\n",
    "De manera adicional, si lo que nos interesa es algún elemento en especial de la página web, podemos hacer clic en **Inspeccionar** en lugar de la opción previamente comentada para localizar esa parte del código en el codigo fuente.\n",
    "\n",
    "Os animo a buscar algún video en YouTube que os explique esto de una manera un poco más visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora veamos como obtener información del codigo fuente, por ejemplo el título\n",
    "soup = BeautifulSoup(source, \"html\") # creamos una variable con la clase BeautifulSoup indicando que nuestro código es html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supongamos por un momento que una página web tiene más de un título.\n",
    "# como hemos visto un par de celdas mas arriba, lo que queremos se encuentra en la etiqueta title\n",
    "\n",
    "titles = soup.find_all('title') # utilizamos la función find_all para encontrar todos los títulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora, para cada título obtenido, imprimamos su valor\n",
    "for title in titles:\n",
    "    print(title.text) # con el atributo text, recuperamos aquello que sea texto (lo que va entre las etiquetas <title></title>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source[7400:7710])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora por ejemplo, hagamos lo mismo para obtener el titulo del artículo y no el de la página wweb.\n",
    "# si hacemos uso de Inspeccionar, veremos que este lo podemos encontrar bajo la etiqueta h1\n",
    "# ademas podemos restringir más la búsqueda para encontrar aquellos atributos pertenecientes a una clase \n",
    "# en particular de la siguiente manera\n",
    "\n",
    "h1s = soup.find_all('h1', attrs={'class': 'firstHeading'})\n",
    "for h1 in h1s:\n",
    "    print(h1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eso es todo a por hoy, a por ello!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Utiliza requests para pedir la página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.elmundo.es/ciencia-y-salud/salud/2020/04/02/5e8569a1fdddff048f8b45d9.html\"\n",
    "req = requests.get(url)\n",
    "source = req.text\n",
    "soup = BeautifulSoup(source, \"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Inspecciona la página para obtener los resultados con BeautifulSoup e imprímelos.\n",
    "\n",
    "**NOTA**: Dispones de información adicional [aquí](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "**NOTA 2**: Es probable que alguna línea tenga espacios por delante o por detrás. Busca una función de la clase [str](https://docs.python.org/3/library/stdtypes.html#text-sequence-type-str) que te ayude a limpiar los espacios del inicio y del final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in soup.find_all('ul', attrs={'class': 'ue-c-article_list ue-c-article_list--unordered'}):\n",
    "    for li in each.find_all('li'):\n",
    "        print(li.text.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
